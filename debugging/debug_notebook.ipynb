{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import lpips\n",
    "\n",
    "\n",
    "def selu_init(m):\n",
    "    \"\"\"\n",
    "    LeCun Normal initialization for selu.\n",
    "    \"\"\"\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"linear\")\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"linear\")\n",
    "        if m.bias is not None:\n",
    "           torch.nn.init.zeros_(m.bias)\n",
    "    if isinstance(m, torch.nn.ConvTranspose2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"linear\")\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class CVAE_v3(pl.LightningModule):\n",
    "    def __init__(self, data, num_domains, num_contents, latent_size, lr, depth, out_channels, kernel_size, activation, downsampling, upsampling, dropout, batch_norm, loss_mode, lamb, no_bn_last=True, initialize=False, max_lamb=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = \"RMNIST\"\n",
    "        self.num_domains = num_domains\n",
    "        self.num_contents = num_contents\n",
    "        self.latent_size = latent_size\n",
    "        self.depth = depth\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.downsampling = downsampling\n",
    "        self.upsampling = upsampling\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        self.loss_mode = loss_mode\n",
    "        self.lamb = lamb\n",
    "        self.no_bn_last = no_bn_last\n",
    "        self.get_mse_loss = torch.nn.MSELoss(reduction=\"mean\")\n",
    "        self.max_lamb = max_lamb\n",
    "        self.hyper_param_dict = {\n",
    "            \"data\": self.data,\n",
    "            \"num_domains\": self.num_domains,\n",
    "            \"num_contents\": self.num_contents,\n",
    "            \"latent_size\": self.latent_size,\n",
    "            \"depth\": self.depth,\n",
    "            \"out_channels\": self.out_channels,\n",
    "            \"kernel_size\": self.kernel_size,\n",
    "            \"activation\": self.activation,\n",
    "            \"downsampling\": self.downsampling,\n",
    "            \"upsampling\": self.upsampling,\n",
    "            \"dropout\": self.dropout,\n",
    "            \"batch_norm\": self.batch_norm,\n",
    "            \"loss_mode\": self.loss_mode,\n",
    "            \"lamb\": self.lamb,\n",
    "            \"no_bn_last\": self.no_bn_last,\n",
    "            \"max_lamb\": self.max_lamb,\n",
    "        }\n",
    "\n",
    "        self.encoder = Encoder(data=self.data,\n",
    "                               num_domains=self.num_domains,\n",
    "                               num_contents=self.num_contents,\n",
    "                               latent_size=self.latent_size,\n",
    "                               depth=self.depth,\n",
    "                               out_channels=self.out_channels,\n",
    "                               kernel_size=self.kernel_size,\n",
    "                               activation=self.activation,\n",
    "                               downsampling=self.downsampling,\n",
    "                               dropout=self.dropout,\n",
    "                               batch_norm=self.batch_norm\n",
    "        )\n",
    "        self.decoder = Decoder(data=self.data,\n",
    "                               num_domains=self.num_domains,\n",
    "                               num_contents=self.num_contents,\n",
    "                               latent_size=self.latent_size,\n",
    "                               depth=self.depth,\n",
    "                               out_channels=self.out_channels[::-1],\n",
    "                               kernel_size=self.kernel_size,\n",
    "                               activation=self.activation,\n",
    "                               upsampling=self.upsampling,\n",
    "                               dropout=self.dropout,\n",
    "                               batch_norm=self.batch_norm,\n",
    "                               no_bn_last=self.no_bn_last\n",
    "        )\n",
    "\n",
    "        self.lr = lr\n",
    "        if initialize:\n",
    "            if isinstance(activation, torch.nn.SELU):\n",
    "                self.apply(selu_init)\n",
    "        if self.loss_mode == \"deep_lpips\":\n",
    "            self.lpips = lpips.LPIPS(net=\"vgg\")\n",
    "        if self.loss_mode == \"deep_own\":\n",
    "            self.own_weight = 0.0\n",
    "\n",
    "    def loss(self, images, enc_mu, enc_logvar, reconstructions, codes_2=None, split_loss=False):\n",
    "        \"\"\"\n",
    "        Calculates the loss. Choose from l1, l2 and elbo\n",
    "\n",
    "        images: Tensor of shape (batch_size, channels, height, width)\n",
    "        enc_mu: Tensor of shape (batch_size, latent_size)\n",
    "        enc_logvar: Tensor of shape (batch_size, latent_size)\n",
    "        reconstructions: Tensor of shape (batch_size, channels, height, width)\n",
    "        split_loss: bool, if True, returns kld and rec losses separately\n",
    "        \"\"\"\n",
    "        if self.loss_mode == \"l1\":\n",
    "            loss = torch.abs(images - reconstructions).mean(dim=[0, 1, 2, 3])\n",
    "            return loss\n",
    "        if self.loss_mode == \"l2\":\n",
    "            loss = self.get_mse_loss(\n",
    "                images, reconstructions)\n",
    "            return loss\n",
    "        if self.loss_mode == \"elbo\":\n",
    "            kld = self.lamb * 0.5 * (enc_mu ** 2 + enc_logvar.exp() - enc_logvar - 1).mean(dim=[0, 1])\n",
    "            rec = self.get_mse_loss(images, reconstructions)\n",
    "            if split_loss:\n",
    "                return kld + rec, kld.item(), rec.item()\n",
    "            else:\n",
    "                return kld + rec\n",
    "        if self.loss_mode == \"l1_elbo\":\n",
    "            kld = self.lamb * 0.5 * (enc_mu ** 2 + enc_logvar.exp() - enc_logvar - 1).mean(dim=[0, 1])\n",
    "            rec = torch.abs(images - reconstructions).mean(dim=[0, 1, 2, 3])\n",
    "            if split_loss:\n",
    "                return kld + rec, kld.item(), rec.item()\n",
    "            else:\n",
    "                return kld + rec\n",
    "        if \"deep\" in self.loss_mode:\n",
    "            if self.loss_mode == \"deep_own\":\n",
    "                img_loss = self.get_mse_loss(images, reconstructions)\n",
    "                code_mu_loss = self.get_mse_loss(enc_mu, codes_2[0])\n",
    "                code_logvar_loss = self.get_mse_loss(enc_logvar, codes_2[1])\n",
    "                self.log(\"deep_loss_img\", img_loss.item(), batch_size=images.shape[0], logger=True)\n",
    "                self.log(\"deep_loss_code_mu\", code_mu_loss.item(), batch_size=images.shape[0], logger=True)\n",
    "                self.log(\"deep_loss_code_logvar\", code_logvar_loss.item(), batch_size=images.shape[0], logger=True)\n",
    "                rec = (1 - self.own_weight) * img_loss + self.own_weight * (code_mu_loss + code_logvar_loss)\n",
    "            elif self.loss_mode == \"deep_lpips\":\n",
    "                rec = self.lpips(images, reconstructions).mean()\n",
    "\n",
    "            kld = self.lamb * 0.5 * (enc_mu ** 2 + enc_logvar.exp() - enc_logvar - 1).mean(dim=[0, 1])\n",
    "\n",
    "            if split_loss:\n",
    "                return kld + rec, kld.item(), rec.item()\n",
    "            else:\n",
    "                return kld + rec\n",
    "\n",
    "    def forward(self, images, domains, contents):\n",
    "        \"\"\"\n",
    "        Calculates codes for the given images and returns their reconstructions.\n",
    "\n",
    "        images: Tensor of shape (batch_size, channels, height, width)\n",
    "        domains: Tensor of shape (batch_size, num_domains)\n",
    "        contents: Tensor of shape (batch_size, num_contents)\n",
    "        \"\"\"\n",
    "        enc_mu, enc_logvar = self.encoder(images, domains, contents)\n",
    "        codes = enc_mu + torch.randn_like(enc_mu) * (0.5 * enc_logvar).exp()\n",
    "        reconstructions = self.decoder(codes, domains, contents)\n",
    "\n",
    "        return enc_mu, enc_logvar, reconstructions\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Calculates the chosen Loss.\n",
    "\n",
    "        batch: List [x, domain, content, filenames]\n",
    "            images: Tensor of shape (batch_size, channels, height, width)\n",
    "            domains: Tensor of shape (batch_size, num_domains)\n",
    "            contents: Tensor of shape (batch_size, num_contents)\n",
    "            filenames: Tuple of strings of the form: {domain}/{content}/{fname}\n",
    "        batch_idx: The index of the batch, not used.\n",
    "        \"\"\"\n",
    "        images = batch[0]\n",
    "        domains = batch[1]\n",
    "        contents = batch[2]\n",
    "\n",
    "        enc_mu, enc_logvar, reconstructions = self(images, domains, contents)\n",
    "\n",
    "        if self.loss_mode == \"deep_own\":\n",
    "            with torch.no_grad():\n",
    "                codes_2 = self.encoder(reconstructions, domains, contents)\n",
    "            loss, kld_value, rec_value = self.loss(images, enc_mu, enc_logvar, reconstructions, codes_2=codes_2, split_loss=True)\n",
    "        elif self.loss_mode == \"deep_lpips\":\n",
    "            loss, kld_value, rec_value = self.loss(images, enc_mu, enc_logvar, reconstructions, split_loss=True)\n",
    "\n",
    "        else:\n",
    "            loss, kld_value, rec_value = self.loss(images, enc_mu, enc_logvar, reconstructions, split_loss=True)\n",
    "        self.log(\"train_loss\", loss, batch_size=images.shape[0])\n",
    "        self.log(\"kld\", kld_value, prog_bar=True, batch_size=images.shape[0])\n",
    "        self.log(\"rec\", rec_value, prog_bar=True, batch_size=images.shape[0])\n",
    "        # self.log(\"lr\", self.optimizers(\n",
    "        # ).param_groups[0][\"lr\"], prog_bar=True, batch_size=images.shape[0])\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Calculates the chosen Loss.\n",
    "\n",
    "        batch: List [x, domain, content, filenames]\n",
    "            images: Tensor of shape (batch_size, channels, height, width)\n",
    "            domains: Tensor of shape (batch_size, num_domains)\n",
    "            contents: Tensor of shape (batch_size, num_contents)\n",
    "            filenames: Tuple of strings of the form: {domain}/{content}/{fname}\n",
    "        batch_idx: The index of the batch, not used.\n",
    "        \"\"\"\n",
    "        images = batch[0]\n",
    "        domains = batch[1]\n",
    "        contents = batch[2]\n",
    "\n",
    "        enc_mu, enc_logvar, reconstructions = self(images, domains, contents)\n",
    "\n",
    "        if self.loss_mode == \"deep_own\":\n",
    "            codes_2 = self.encoder(reconstructions, domains, contents)\n",
    "            loss = self.loss(images, enc_mu, enc_logvar, reconstructions, codes_2=codes_2)\n",
    "        elif self.loss_mode == \"deep_lpips\":\n",
    "            loss = self.loss(images, enc_mu, enc_logvar, reconstructions)\n",
    "        else:\n",
    "            loss = self.loss(images, enc_mu, enc_logvar, reconstructions)\n",
    "        self.log(\"val_loss\", loss, batch_size=images.shape[0])\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Calculates the chosen Loss.\n",
    "\n",
    "        batch: List [x, domain, content, filenames]\n",
    "            images: Tensor of shape (batch_size, channels, height, width)\n",
    "            domains: Tensor of shape (batch_size, num_domains)\n",
    "            contents: Tensor of shape (batch_size, num_contents)\n",
    "            filenames: Tuple of strings of the form: {domain}/{content}/{fname}\n",
    "        batch_idx: The index of the batch, not used.\n",
    "        \"\"\"\n",
    "        images = batch[0]\n",
    "        domains = batch[1]\n",
    "        contents = batch[2]\n",
    "\n",
    "        enc_mu, enc_logvar, reconstructions = self(images, domains, contents)\n",
    "\n",
    "        return self.loss(images, enc_mu, enc_logvar, reconstructions)\n",
    "\n",
    "    def configure_optimizers(self, reduce_lr=False):\n",
    "        if reduce_lr:\n",
    "            self.lr /= 10 ** 0.5\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def warmer(self):\n",
    "        if self.lamb < self.max_lamb:\n",
    "            self.lamb *= 10 ** 0.5\n",
    "            print(f\"New lambda: {self.lamb}\")\n",
    "            if self.loss_mode == \"deep_own\":\n",
    "                if self.own_weight == 0.0:\n",
    "                    self.own_weight = 0.1\n",
    "                elif self.own_weight < 1.0:\n",
    "                    self.own_weight *= 10 ** 0.5\n",
    "                print(f\"New own_weight: {self.own_weight}\")\n",
    "\n",
    "    def reconstruct(self, images, domains, contents):\n",
    "        \"\"\"\n",
    "        Calculates codes for the given images and returns their reconstructions.\n",
    "\n",
    "        images: Tensor of shape (batch_size, channels, height, width)\n",
    "        domains: Tensor of shape (batch_size, num_domains)\n",
    "        contents: Tensor of shape (batch_size, num_contents)\n",
    "        \"\"\"\n",
    "        enc_mu, enc_logvar = self.encoder(images, domains, contents)\n",
    "        reconstructions = self.decoder(enc_mu, domains, contents)\n",
    "\n",
    "        return reconstructions\n",
    "\n",
    "    def transfer(self, images, domains, contents, decoder_domains, decoder_contents):\n",
    "        \"\"\"\n",
    "        Calculates codes for the given images and returns their reconstructions.\n",
    "\n",
    "        images: Tensor of shape (batch_size, channels, height, width)\n",
    "        domains: Tensor of shape (batch_size, num_domains)\n",
    "        contents: Tensor of shape (batch_size, num_contents)\n",
    "        decoder_domains: Tensor of shape (batch_size, num_domains)\n",
    "        decoder_contents: Tensor of shape (batch_size, num_contents)\n",
    "        \"\"\"\n",
    "        enc_mu, enc_logvar = self.encoder(images, domains, contents)\n",
    "        transfers = self.decoder(enc_mu, decoder_domains, decoder_contents)\n",
    "\n",
    "        return transfers\n",
    "\n",
    "    def generate(self, codes, domains, contents):\n",
    "        \"\"\"\n",
    "        Generate images from Gaussian distributed codes.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            x = torch.cat((codes, domains, contents), dim=1)\n",
    "            x = self.decoder.linear(x)\n",
    "            x = self.decoder.reshape(x)\n",
    "            reconstructions = self.decoder.dec_conv_sequential(x)\n",
    "            self.train()\n",
    "            return reconstructions\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, data, num_domains, num_contents, latent_size, depth, out_channels, kernel_size, activation, downsampling, dropout, batch_norm):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.HW = {\"PACS\": 224, \"RMNIST\": 28}[self.data]\n",
    "        self.C = {\"PACS\": 3, \"RMNIST\": 1}[self.data]\n",
    "        self.num_domains = num_domains\n",
    "        self.num_contents = num_contents\n",
    "        self.latent_size = latent_size\n",
    "        self.depth = depth\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.downsampling = downsampling\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        if self.data == \"PACS\":\n",
    "            self.enc_conv_sequential = torch.nn.Sequential(\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.C + self.num_domains + self.num_contents,\n",
    "                    out_channels=self.out_channels[0],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=\"none\",\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [0], 224, 224)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[0],\n",
    "                    out_channels=self.out_channels[1],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=self.downsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [1], 112, 112)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[1],\n",
    "                    out_channels=self.out_channels[2],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=self.downsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [2], 56, 56)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[2],\n",
    "                    out_channels=self.out_channels[3],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=self.downsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [3], 28, 28)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[3],\n",
    "                    out_channels=self.out_channels[4],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=self.downsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [4], 14, 14)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[4],\n",
    "                    out_channels=self.out_channels[5],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=self.downsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [5], 7, 7)\n",
    "            )\n",
    "            self.flatten = torch.nn.Flatten()\n",
    "            self.get_mu = torch.nn.Linear(49 * self.out_channels[5], self.latent_size)\n",
    "            self.get_logvar = torch.nn.Linear(49 * self.out_channels[5], self.latent_size)\n",
    "        if self.data == \"RMNIST\":\n",
    "            self.enc_conv_sequential = torch.nn.Sequential(\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.C + self.num_domains + self.num_contents,\n",
    "                    out_channels=self.out_channels[0],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=\"none\",\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [0], 28, 28)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[0],\n",
    "                    out_channels=self.out_channels[1],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=self.downsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [1], 14, 14)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[1],\n",
    "                    out_channels=self.out_channels[2],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=self.downsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [2], 7, 7)\n",
    "            )\n",
    "            self.flatten = torch.nn.Flatten()\n",
    "            self.get_mu = torch.nn.Linear(49 * self.out_channels[2], self.latent_size)\n",
    "            self.get_logvar = torch.nn.Linear(49 * self.out_channels[2], self.latent_size)\n",
    "\n",
    "    def block(self, depth, in_channels, out_channels, kernel_size, activation, downsampling=\"stride\", dropout=False, batch_norm=False):\n",
    "        seq_list = []\n",
    "        if isinstance(activation, torch.nn.SELU):\n",
    "            dropout = False\n",
    "            batch_norm = False\n",
    "        for i in range(depth):\n",
    "            seq = []\n",
    "            if i == 0: # downsampling in first layer of block\n",
    "                if downsampling == \"stride\":\n",
    "                    seq.append(torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                        padding=int((kernel_size-1)/2), stride=2, bias=not batch_norm))\n",
    "                    if batch_norm:\n",
    "                        seq.append(torch.nn.BatchNorm2d(num_features=out_channels))\n",
    "                    seq.append(activation)\n",
    "                    if dropout:\n",
    "                        seq.append(torch.nn.Dropout2d())\n",
    "                    seq_list += seq\n",
    "                elif downsampling == \"maxpool\":\n",
    "                    seq.append(torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                        padding=int((kernel_size-1)/2), stride=1, bias=not batch_norm))\n",
    "                    if batch_norm:\n",
    "                        seq.append(torch.nn.BatchNorm2d(num_features=out_channels))\n",
    "                    seq.append(activation)\n",
    "                    seq.append(torch.nn.MaxPool2d(kernel_size=2))\n",
    "                    if dropout:\n",
    "                        seq.append(torch.nn.Dropout2d())\n",
    "                    seq_list += seq\n",
    "                elif downsampling == \"none\":\n",
    "                    seq.append(torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                        padding=int((kernel_size-1)/2), stride=1, bias=not batch_norm))\n",
    "                    if batch_norm:\n",
    "                        seq.append(torch.nn.BatchNorm2d(num_features=out_channels))\n",
    "                    seq.append(activation)\n",
    "                    if dropout:\n",
    "                        seq.append(torch.nn.Dropout2d())\n",
    "                    seq_list += seq\n",
    "            else:\n",
    "                seq.append(torch.nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                    padding=int((kernel_size-1)/2), stride=1, bias=not batch_norm))\n",
    "                if batch_norm:\n",
    "                    seq.append(torch.nn.BatchNorm2d(num_features=out_channels))\n",
    "                seq.append(activation)\n",
    "                if dropout:\n",
    "                    seq.append(torch.nn.Dropout2d())\n",
    "                seq_list += seq\n",
    "        return seq_list\n",
    "\n",
    "    def forward(self, images, domains, contents):\n",
    "        \"\"\"\n",
    "        Calculates latent-space encodings for the given images in the form p(z | x).\n",
    "\n",
    "        images: Tensor of shape (batch_size, channels, height, width)\n",
    "        domains: Tensor of shape (batch_size, num_domains)\n",
    "        contents: Tensor of shape (batch_size, num_contents)\n",
    "        \"\"\"\n",
    "        domain_panels = torch.ones(size=(images.shape[0], self.num_domains, self.HW, self.HW)).to(\n",
    "            images.device) * domains.view(images.shape[0], self.num_domains, 1, 1)\n",
    "        content_panels = torch.ones(size=(images.shape[0], self.num_contents, self.HW, self.HW)).to(\n",
    "            images.device) * contents.view(images.shape[0], self.num_contents, 1, 1)\n",
    "\n",
    "        x = torch.cat((images, domain_panels, content_panels), dim=1)\n",
    "        x = self.enc_conv_sequential(x)\n",
    "        x = self.flatten(x)\n",
    "        enc_mu = self.get_mu(x)\n",
    "        enc_logvar = self.get_logvar(x)\n",
    "\n",
    "        return enc_mu, enc_logvar\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, data, num_domains, num_contents, latent_size, depth, out_channels, kernel_size, activation, upsampling, dropout, batch_norm, no_bn_last):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.HW = {\"PACS\": 224, \"RMNIST\": 28}[self.data]\n",
    "        self.C = {\"PACS\": 3, \"RMNIST\": 1}[self.data]\n",
    "        self.num_domains = num_domains\n",
    "        self.num_contents = num_contents\n",
    "        self.latent_size = latent_size\n",
    "        self.depth = depth\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.upsampling = upsampling\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        self.no_bn_last = no_bn_last\n",
    "        self.linear = torch.nn.Linear(\n",
    "            self.latent_size + self.num_domains + self.num_contents, 49 * self.out_channels[0])\n",
    "        if self.data == \"PACS\":\n",
    "            self.dec_conv_sequential = torch.nn.Sequential(\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[0],\n",
    "                    out_channels=self.out_channels[1],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=\"none\",\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm,\n",
    "                ),  # (N, [1], 7, 7)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[1],\n",
    "                    out_channels=self.out_channels[2],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=self.upsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm,\n",
    "                ),  # (N, [2], 14, 14)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[2],\n",
    "                    out_channels=self.out_channels[3],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=self.upsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [3], 28, 28)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[3],\n",
    "                    out_channels=self.out_channels[4],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=self.upsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [4], 56, 56)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[4],\n",
    "                    out_channels=self.out_channels[5],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=self.upsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [5], 112, 112)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[5],\n",
    "                    out_channels=self.C,\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=self.upsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm,\n",
    "                    last_block=self.no_bn_last\n",
    "                ),  # (N, self.C, 224, 224)\n",
    "            )\n",
    "        if self.data == \"RMNIST\":\n",
    "            self.dec_conv_sequential = torch.nn.Sequential(\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[0],\n",
    "                    out_channels=self.out_channels[1],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=\"none\",\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [1], 7, 7)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[1],\n",
    "                    out_channels=self.out_channels[2],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=self.upsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [2], 14, 14)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[2],\n",
    "                    out_channels=self.C,\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=self.upsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm,\n",
    "                    last_block=self.no_bn_last\n",
    "                ),  # (N, self.C, 28, 28)\n",
    "            )\n",
    "\n",
    "    def block(self, depth, in_channels, out_channels, kernel_size, activation, upsampling=\"stride\", dropout=False, batch_norm=False, last_block=False):\n",
    "        seq_list = []\n",
    "        if isinstance(activation, torch.nn.SELU):\n",
    "            dropout = False\n",
    "            batch_norm = False\n",
    "        for i in range(depth):\n",
    "            seq = []\n",
    "            if i == 0: # upsampling in first layer of block\n",
    "                if upsampling == \"stride\":\n",
    "                    seq.append(torch.nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=4,\n",
    "                                        padding=int((kernel_size-1)/2), output_padding=0, stride=2, bias=not batch_norm))\n",
    "                    if batch_norm:\n",
    "                        if not (i == depth - 1 and last_block):\n",
    "                            seq.append(torch.nn.BatchNorm2d(num_features=out_channels))\n",
    "                    seq.append(activation)\n",
    "                    if dropout:\n",
    "                        if not (i == depth - 1 and last_block):\n",
    "                            seq.append(torch.nn.Dropout2d())\n",
    "                    seq_list += seq\n",
    "                elif upsampling == \"upsample\":\n",
    "                    seq.append(torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                        padding=int((kernel_size-1)/2), stride=1, bias=not batch_norm))\n",
    "                    if batch_norm:\n",
    "                        if not (i == depth - 1 and last_block):\n",
    "                            seq.append(torch.nn.BatchNorm2d(num_features=out_channels))\n",
    "                    seq.append(activation)\n",
    "                    seq.append(torch.nn.Upsample(scale_factor=2, mode=\"nearest\"))\n",
    "                    if dropout:\n",
    "                        if not (i == depth - 1 and last_block):\n",
    "                            seq.append(torch.nn.Dropout2d())\n",
    "                    seq_list += seq\n",
    "                elif upsampling == \"none\":\n",
    "                    seq.append(torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                        padding=int((kernel_size-1)/2), stride=1, bias=not batch_norm))\n",
    "                    if batch_norm:\n",
    "                        if not (i == depth - 1 and last_block):\n",
    "                            seq.append(torch.nn.BatchNorm2d(num_features=out_channels))\n",
    "                    seq.append(activation)\n",
    "                    if dropout:\n",
    "                        if not (i == depth - 1 and last_block):\n",
    "                            seq.append(torch.nn.Dropout2d())\n",
    "                    seq_list += seq\n",
    "            else:\n",
    "                seq.append(torch.nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                    padding=int((kernel_size-1)/2), stride=1, bias=not batch_norm))\n",
    "                if batch_norm:\n",
    "                    if not (i == depth - 1 and last_block):\n",
    "                        seq.append(torch.nn.BatchNorm2d(num_features=out_channels))\n",
    "                seq.append(activation)\n",
    "                if dropout:\n",
    "                    if not (i == depth - 1 and last_block):\n",
    "                        seq.append(torch.nn.Dropout2d())\n",
    "                seq_list += seq\n",
    "        return seq_list\n",
    "\n",
    "    def reshape(self, x):\n",
    "        return x.view(-1, self.out_channels[0], 7, 7)\n",
    "\n",
    "    def forward(self, codes, domains, contents):\n",
    "        \"\"\"\n",
    "        Calculates reconstructions of the given latent-space encodings.\n",
    "\n",
    "        codes: Tensor of shape (batch_size, latent_size)\n",
    "        domains: Tensor of shape (batch_size, num_domains)\n",
    "        contents: Tensor of shape (batch_size, num_contents)\n",
    "        \"\"\"\n",
    "        x = torch.cat((codes, domains, contents), dim=1)\n",
    "        x = self.linear(x)\n",
    "        x = self.reshape(x)\n",
    "        reconstructions = self.dec_conv_sequential(x)\n",
    "        return reconstructions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class SetToTanhRange(object):\n",
    "    \"\"\"\n",
    "    Shift torch.Tensor from [0, 1] to [-1, 1] range.\n",
    "    \"\"\"\n",
    "    def __call__(self, sample):\n",
    "        return 2.0 * sample - 1.0\n",
    "\n",
    "    \n",
    "def collate_function(batch):\n",
    "    images = []\n",
    "    domains = []\n",
    "    contents = []\n",
    "    for i in range(batch[0][1][0].shape[0]):\n",
    "        for minibatch in batch:\n",
    "            images.append(minibatch[0][i])\n",
    "            domains.append(minibatch[1][i])\n",
    "            contents.append(minibatch[2][i])\n",
    "    images = torch.stack(images)\n",
    "    domains = torch.stack(domains)\n",
    "    contents = torch.stack(contents)\n",
    "    return images, domains, contents\n",
    "\n",
    "\n",
    "class RMNISTDataset(Dataset):\n",
    "    def __init__(self, root, mode, domains, contents):\n",
    "        \"\"\"\n",
    "        root: str, root folder where RMNIST is located\n",
    "        mode: str, choose one: \"train\", \"val\" or \"test\"\n",
    "        domains: list of int [0, 15, 30, 45, 60, 75]\n",
    "        contents: list of int [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.domains = domains\n",
    "        self.contents = contents\n",
    "        self.domain_dict = {domain: torch.LongTensor([i]) for i, domain in enumerate(self.domains)}\n",
    "        self.content_dict = {content: torch.LongTensor([i]) for i, content in enumerate(self.contents)}\n",
    "        self.data_dir = f\"../{root}/RMNIST_{mode}\"\n",
    "        self.image_data = {domain: [] for domain in self.domains}\n",
    "        self.domain_data = {domain: [] for domain in self.domains}\n",
    "        self.content_data = {domain: [] for domain in self.domains}\n",
    "        for domain in os.listdir(f\"{self.data_dir}\"):\n",
    "            domain = int(domain)\n",
    "            image_data_ = ()\n",
    "            domain_data_ = ()\n",
    "            content_data_ = ()\n",
    "            if domain in self.domains:\n",
    "                for content in os.listdir(f\"{self.data_dir}/{domain}\"):\n",
    "                    content = int(content)\n",
    "                    if content in self.contents:\n",
    "                        imgs = torch.load(f\"{self.data_dir}/{domain}/{content}/data.pt\")\n",
    "                        image_data_ += (imgs,)\n",
    "                        domain_data_ += (torch.nn.functional.one_hot(self.domain_dict[domain], num_classes=len(self.domains)).view(1, -1),) * imgs.shape[0]\n",
    "                        content_data_ += (torch.nn.functional.one_hot(self.content_dict[content], num_classes=len(self.contents)).view(1, -1),) * imgs.shape[0]\n",
    "                image_data_ = torch.cat(image_data_, dim=0)\n",
    "                domain_data_ = torch.cat(domain_data_, dim=0)\n",
    "                content_data_ = torch.cat(content_data_, dim=0)\n",
    "                torch.manual_seed(17 + domain)\n",
    "                shuffle_inds = torch.randperm(len(image_data_))\n",
    "                image_data_ = image_data_[shuffle_inds]\n",
    "                domain_data_ = domain_data_[shuffle_inds]\n",
    "                content_data_ = content_data_[shuffle_inds]\n",
    "                self.image_data[domain] = image_data_\n",
    "                self.domain_data[domain] = domain_data_\n",
    "                self.content_data[domain] = content_data_\n",
    "        self.transform = self.get_transform()\n",
    "\n",
    "    def get_transform(self):\n",
    "        transform = transforms.Compose([\n",
    "            SetToTanhRange(),\n",
    "        ])\n",
    "        return transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return max([len(self.image_data[domain]) for domain in self.domains])\n",
    "\n",
    "    def __getitem__(self, idx):    \n",
    "        images = []\n",
    "        domains = []\n",
    "        contents = []\n",
    "        for d in self.domains:\n",
    "            if idx >= len(self.image_data[d]):\n",
    "                idx = torch.randint(low=0, high=len(self.image_data[d]), size=(1,)).item()\n",
    "            image = self.transform(self.image_data[d][idx])\n",
    "            domain = self.domain_data[d][idx]\n",
    "            content = self.content_data[d][idx]\n",
    "            images.append(image)\n",
    "            domains.append(domain)\n",
    "            contents.append(content)\n",
    "        return images, domains, contents\n",
    "\n",
    "        \n",
    "class BalancedRMNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, root, domains, contents, batch_size, num_workers, shuffle_all=False):\n",
    "        \"\"\"\n",
    "        root: str, root folder where RMNIST is located\n",
    "        domains: list of int [0, 15, 30, 45, 60, 75]\n",
    "        contents: list of int [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        batch_size: int, batch_size to use for the dataloaders\n",
    "        num_workers: int, how many workers to use for the dataloader\n",
    "        shuffle_all: bool, if True val and test dataloaders are shuffled as well\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.domains = domains\n",
    "        self.contents = contents\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.shuffle_all = shuffle_all\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage in (None, \"fit\"):\n",
    "            self.rmnist_train = RMNISTDataset(root=self.root, mode=\"train\", domains=self.domains, contents=self.contents)\n",
    "            self.rmnist_val = RMNISTDataset(root=self.root, mode=\"val\", domains=self.domains, contents=self.contents)\n",
    "        if stage in (None, \"test\"):\n",
    "            self.rmnist_test = RMNISTDataset(root=self.root, mode=\"test\", domains=self.domains, contents=self.contents)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.rmnist_train, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, collate_fn=collate_function)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.rmnist_val, batch_size=self.batch_size, shuffle=self.shuffle_all, num_workers=self.num_workers, collate_fn=collate_function)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.rmnist_test, batch_size=self.batch_size, shuffle=self.shuffle_all, num_workers=self.num_workers, collate_fn=collate_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"../logs/experiment/rmnist_meta4/version_0/checkpoints/last.ckpt\"\n",
    "data = \"RMNIST\"\n",
    "num_domains = 5\n",
    "num_contents = 10\n",
    "latent_size = 512\n",
    "lr = 1e-4\n",
    "depth = 2\n",
    "out_channels = [256, 512, 1024]\n",
    "kernel_size = 3\n",
    "activation = torch.nn.ELU()\n",
    "downsampling = \"stride\"\n",
    "upsampling = \"upsample\"\n",
    "dropout = False\n",
    "batch_norm = True\n",
    "loss_mode = \"elbo\"\n",
    "lamb = 0.01\n",
    "no_bn_last = True\n",
    "\n",
    "\n",
    "\n",
    "model = CVAE_v3.load_from_checkpoint(ckpt_path, num_domains=num_domains, num_contents=num_contents,\n",
    "                                     latent_size=latent_size, lr=lr, depth=depth, out_channels=out_channels,\n",
    "                                     kernel_size=kernel_size, activation=activation, downsampling=downsampling,\n",
    "                                     upsampling=upsampling, dropout=dropout, batch_norm=batch_norm, loss_mode=loss_mode,\n",
    "                                     lamb=lamb, no_bn_last=no_bn_last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = [0, 15, 30, 45, 60, 75] \n",
    "contents = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \n",
    "dm = BalancedRMNISTDataModule(root=\"data\", domains=domains, contents=contents, batch_size=2, num_workers=0)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dm.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = batch[0]\n",
    "domains = batch[1]\n",
    "contents = batch[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "        [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "        [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "        [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "        [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "        [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]]])\n"
     ]
    }
   ],
   "source": [
    "fours = torch.zeros(size=(6, 28, 28))\n",
    "for batch in iter(dm.train_dataloader()):\n",
    "    images = batch[0]\n",
    "    domains = batch[1]\n",
    "    contents = batch[2]\n",
    "    for i, c in enumerate(contents):\n",
    "        if torch.argmax(c).item() == 4:\n",
    "            # print(torch.argmax(domains[i]))\n",
    "            fours[torch.argmax(domains[i]).item()] = images[i].view(28, 28)\n",
    "print(fours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.gray()\n",
    "for i, f in enumerate(fours):\n",
    "    plt.subplot(1, 6, i+1)\n",
    "    plt.imshow(f)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7930ccaf1029f9b8696599c8d114a3df4eb78a4984b726cde572031570791305"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('baver1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
