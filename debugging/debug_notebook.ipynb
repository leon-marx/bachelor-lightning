{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import lpips\n",
    "\n",
    "\n",
    "def selu_init(m):\n",
    "    \"\"\"\n",
    "    LeCun Normal initialization for selu.\n",
    "    \"\"\"\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"linear\")\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"linear\")\n",
    "        if m.bias is not None:\n",
    "           torch.nn.init.zeros_(m.bias)\n",
    "    if isinstance(m, torch.nn.ConvTranspose2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"linear\")\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class CVAE_v3(pl.LightningModule):\n",
    "    def __init__(self, data, num_domains, num_contents, latent_size, lr, depth, out_channels, kernel_size, activation, downsampling, upsampling, dropout, batch_norm, loss_mode, lamb, no_bn_last=True, initialize=False, max_lamb=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = \"RMNIST\"\n",
    "        self.num_domains = num_domains\n",
    "        self.num_contents = num_contents\n",
    "        self.latent_size = latent_size\n",
    "        self.depth = depth\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.downsampling = downsampling\n",
    "        self.upsampling = upsampling\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        self.loss_mode = loss_mode\n",
    "        self.lamb = lamb\n",
    "        self.no_bn_last = no_bn_last\n",
    "        self.get_mse_loss = torch.nn.MSELoss(reduction=\"mean\")\n",
    "        self.max_lamb = max_lamb\n",
    "        self.hyper_param_dict = {\n",
    "            \"data\": self.data,\n",
    "            \"num_domains\": self.num_domains,\n",
    "            \"num_contents\": self.num_contents,\n",
    "            \"latent_size\": self.latent_size,\n",
    "            \"depth\": self.depth,\n",
    "            \"out_channels\": self.out_channels,\n",
    "            \"kernel_size\": self.kernel_size,\n",
    "            \"activation\": self.activation,\n",
    "            \"downsampling\": self.downsampling,\n",
    "            \"upsampling\": self.upsampling,\n",
    "            \"dropout\": self.dropout,\n",
    "            \"batch_norm\": self.batch_norm,\n",
    "            \"loss_mode\": self.loss_mode,\n",
    "            \"lamb\": self.lamb,\n",
    "            \"no_bn_last\": self.no_bn_last,\n",
    "            \"max_lamb\": self.max_lamb,\n",
    "        }\n",
    "\n",
    "        self.encoder = Encoder(data=self.data,\n",
    "                               num_domains=self.num_domains,\n",
    "                               num_contents=self.num_contents,\n",
    "                               latent_size=self.latent_size,\n",
    "                               depth=self.depth,\n",
    "                               out_channels=self.out_channels,\n",
    "                               kernel_size=self.kernel_size,\n",
    "                               activation=self.activation,\n",
    "                               downsampling=self.downsampling,\n",
    "                               dropout=self.dropout,\n",
    "                               batch_norm=self.batch_norm\n",
    "        )\n",
    "        self.decoder = Decoder(data=self.data,\n",
    "                               num_domains=self.num_domains,\n",
    "                               num_contents=self.num_contents,\n",
    "                               latent_size=self.latent_size,\n",
    "                               depth=self.depth,\n",
    "                               out_channels=self.out_channels[::-1],\n",
    "                               kernel_size=self.kernel_size,\n",
    "                               activation=self.activation,\n",
    "                               upsampling=self.upsampling,\n",
    "                               dropout=self.dropout,\n",
    "                               batch_norm=self.batch_norm,\n",
    "                               no_bn_last=self.no_bn_last\n",
    "        )\n",
    "\n",
    "        self.lr = lr\n",
    "        if initialize:\n",
    "            if isinstance(activation, torch.nn.SELU):\n",
    "                self.apply(selu_init)\n",
    "        if self.loss_mode == \"deep_lpips\":\n",
    "            self.lpips = lpips.LPIPS(net=\"vgg\")\n",
    "        if self.loss_mode == \"deep_own\":\n",
    "            self.own_weight = 0.0\n",
    "\n",
    "    def loss(self, images, enc_mu, enc_logvar, reconstructions, codes_2=None, split_loss=False):\n",
    "        \"\"\"\n",
    "        Calculates the loss. Choose from l1, l2 and elbo\n",
    "\n",
    "        images: Tensor of shape (batch_size, channels, height, width)\n",
    "        enc_mu: Tensor of shape (batch_size, latent_size)\n",
    "        enc_logvar: Tensor of shape (batch_size, latent_size)\n",
    "        reconstructions: Tensor of shape (batch_size, channels, height, width)\n",
    "        split_loss: bool, if True, returns kld and rec losses separately\n",
    "        \"\"\"\n",
    "        if self.loss_mode == \"l1\":\n",
    "            loss = torch.abs(images - reconstructions).mean(dim=[0, 1, 2, 3])\n",
    "            return loss\n",
    "        if self.loss_mode == \"l2\":\n",
    "            loss = self.get_mse_loss(\n",
    "                images, reconstructions)\n",
    "            return loss\n",
    "        if self.loss_mode == \"elbo\":\n",
    "            kld = self.lamb * 0.5 * (enc_mu ** 2 + enc_logvar.exp() - enc_logvar - 1).mean(dim=[0, 1])\n",
    "            rec = self.get_mse_loss(images, reconstructions)\n",
    "            if split_loss:\n",
    "                return kld + rec, kld.item(), rec.item()\n",
    "            else:\n",
    "                return kld + rec\n",
    "        if self.loss_mode == \"l1_elbo\":\n",
    "            kld = self.lamb * 0.5 * (enc_mu ** 2 + enc_logvar.exp() - enc_logvar - 1).mean(dim=[0, 1])\n",
    "            rec = torch.abs(images - reconstructions).mean(dim=[0, 1, 2, 3])\n",
    "            if split_loss:\n",
    "                return kld + rec, kld.item(), rec.item()\n",
    "            else:\n",
    "                return kld + rec\n",
    "        if \"deep\" in self.loss_mode:\n",
    "            if self.loss_mode == \"deep_own\":\n",
    "                img_loss = self.get_mse_loss(images, reconstructions)\n",
    "                code_mu_loss = self.get_mse_loss(enc_mu, codes_2[0])\n",
    "                code_logvar_loss = self.get_mse_loss(enc_logvar, codes_2[1])\n",
    "                self.log(\"deep_loss_img\", img_loss.item(), batch_size=images.shape[0], logger=True)\n",
    "                self.log(\"deep_loss_code_mu\", code_mu_loss.item(), batch_size=images.shape[0], logger=True)\n",
    "                self.log(\"deep_loss_code_logvar\", code_logvar_loss.item(), batch_size=images.shape[0], logger=True)\n",
    "                rec = (1 - self.own_weight) * img_loss + self.own_weight * (code_mu_loss + code_logvar_loss)\n",
    "            elif self.loss_mode == \"deep_lpips\":\n",
    "                rec = self.lpips(images, reconstructions).mean()\n",
    "\n",
    "            kld = self.lamb * 0.5 * (enc_mu ** 2 + enc_logvar.exp() - enc_logvar - 1).mean(dim=[0, 1])\n",
    "\n",
    "            if split_loss:\n",
    "                return kld + rec, kld.item(), rec.item()\n",
    "            else:\n",
    "                return kld + rec\n",
    "\n",
    "    def forward(self, images, domains, contents):\n",
    "        \"\"\"\n",
    "        Calculates codes for the given images and returns their reconstructions.\n",
    "\n",
    "        images: Tensor of shape (batch_size, channels, height, width)\n",
    "        domains: Tensor of shape (batch_size, num_domains)\n",
    "        contents: Tensor of shape (batch_size, num_contents)\n",
    "        \"\"\"\n",
    "        enc_mu, enc_logvar = self.encoder(images, domains, contents)\n",
    "        codes = enc_mu + torch.randn_like(enc_mu) * (0.5 * enc_logvar).exp()\n",
    "        reconstructions = self.decoder(codes, domains, contents)\n",
    "\n",
    "        return enc_mu, enc_logvar, reconstructions\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Calculates the chosen Loss.\n",
    "\n",
    "        batch: List [x, domain, content, filenames]\n",
    "            images: Tensor of shape (batch_size, channels, height, width)\n",
    "            domains: Tensor of shape (batch_size, num_domains)\n",
    "            contents: Tensor of shape (batch_size, num_contents)\n",
    "            filenames: Tuple of strings of the form: {domain}/{content}/{fname}\n",
    "        batch_idx: The index of the batch, not used.\n",
    "        \"\"\"\n",
    "        images = batch[0]\n",
    "        domains = batch[1]\n",
    "        contents = batch[2]\n",
    "\n",
    "        enc_mu, enc_logvar, reconstructions = self(images, domains, contents)\n",
    "\n",
    "        if self.loss_mode == \"deep_own\":\n",
    "            with torch.no_grad():\n",
    "                codes_2 = self.encoder(reconstructions, domains, contents)\n",
    "            loss, kld_value, rec_value = self.loss(images, enc_mu, enc_logvar, reconstructions, codes_2=codes_2, split_loss=True)\n",
    "        elif self.loss_mode == \"deep_lpips\":\n",
    "            loss, kld_value, rec_value = self.loss(images, enc_mu, enc_logvar, reconstructions, split_loss=True)\n",
    "\n",
    "        else:\n",
    "            loss, kld_value, rec_value = self.loss(images, enc_mu, enc_logvar, reconstructions, split_loss=True)\n",
    "        self.log(\"train_loss\", loss, batch_size=images.shape[0])\n",
    "        self.log(\"kld\", kld_value, prog_bar=True, batch_size=images.shape[0])\n",
    "        self.log(\"rec\", rec_value, prog_bar=True, batch_size=images.shape[0])\n",
    "        # self.log(\"lr\", self.optimizers(\n",
    "        # ).param_groups[0][\"lr\"], prog_bar=True, batch_size=images.shape[0])\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Calculates the chosen Loss.\n",
    "\n",
    "        batch: List [x, domain, content, filenames]\n",
    "            images: Tensor of shape (batch_size, channels, height, width)\n",
    "            domains: Tensor of shape (batch_size, num_domains)\n",
    "            contents: Tensor of shape (batch_size, num_contents)\n",
    "            filenames: Tuple of strings of the form: {domain}/{content}/{fname}\n",
    "        batch_idx: The index of the batch, not used.\n",
    "        \"\"\"\n",
    "        images = batch[0]\n",
    "        domains = batch[1]\n",
    "        contents = batch[2]\n",
    "\n",
    "        enc_mu, enc_logvar, reconstructions = self(images, domains, contents)\n",
    "\n",
    "        if self.loss_mode == \"deep_own\":\n",
    "            codes_2 = self.encoder(reconstructions, domains, contents)\n",
    "            loss = self.loss(images, enc_mu, enc_logvar, reconstructions, codes_2=codes_2)\n",
    "        elif self.loss_mode == \"deep_lpips\":\n",
    "            loss = self.loss(images, enc_mu, enc_logvar, reconstructions)\n",
    "        else:\n",
    "            loss = self.loss(images, enc_mu, enc_logvar, reconstructions)\n",
    "        self.log(\"val_loss\", loss, batch_size=images.shape[0])\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Calculates the chosen Loss.\n",
    "\n",
    "        batch: List [x, domain, content, filenames]\n",
    "            images: Tensor of shape (batch_size, channels, height, width)\n",
    "            domains: Tensor of shape (batch_size, num_domains)\n",
    "            contents: Tensor of shape (batch_size, num_contents)\n",
    "            filenames: Tuple of strings of the form: {domain}/{content}/{fname}\n",
    "        batch_idx: The index of the batch, not used.\n",
    "        \"\"\"\n",
    "        images = batch[0]\n",
    "        domains = batch[1]\n",
    "        contents = batch[2]\n",
    "\n",
    "        enc_mu, enc_logvar, reconstructions = self(images, domains, contents)\n",
    "\n",
    "        return self.loss(images, enc_mu, enc_logvar, reconstructions)\n",
    "\n",
    "    def configure_optimizers(self, reduce_lr=False):\n",
    "        if reduce_lr:\n",
    "            self.lr /= 10 ** 0.5\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def warmer(self):\n",
    "        if self.lamb < self.max_lamb:\n",
    "            self.lamb *= 10 ** 0.5\n",
    "            print(f\"New lambda: {self.lamb}\")\n",
    "            if self.loss_mode == \"deep_own\":\n",
    "                if self.own_weight == 0.0:\n",
    "                    self.own_weight = 0.1\n",
    "                elif self.own_weight < 1.0:\n",
    "                    self.own_weight *= 10 ** 0.5\n",
    "                print(f\"New own_weight: {self.own_weight}\")\n",
    "\n",
    "    def reconstruct(self, images, domains, contents):\n",
    "        \"\"\"\n",
    "        Calculates codes for the given images and returns their reconstructions.\n",
    "\n",
    "        images: Tensor of shape (batch_size, channels, height, width)\n",
    "        domains: Tensor of shape (batch_size, num_domains)\n",
    "        contents: Tensor of shape (batch_size, num_contents)\n",
    "        \"\"\"\n",
    "        enc_mu, enc_logvar = self.encoder(images, domains, contents)\n",
    "        reconstructions = self.decoder(enc_mu, domains, contents)\n",
    "\n",
    "        return reconstructions\n",
    "\n",
    "    def transfer(self, images, domains, contents, decoder_domains, decoder_contents):\n",
    "        \"\"\"\n",
    "        Calculates codes for the given images and returns their reconstructions.\n",
    "\n",
    "        images: Tensor of shape (batch_size, channels, height, width)\n",
    "        domains: Tensor of shape (batch_size, num_domains)\n",
    "        contents: Tensor of shape (batch_size, num_contents)\n",
    "        decoder_domains: Tensor of shape (batch_size, num_domains)\n",
    "        decoder_contents: Tensor of shape (batch_size, num_contents)\n",
    "        \"\"\"\n",
    "        enc_mu, enc_logvar = self.encoder(images, domains, contents)\n",
    "        transfers = self.decoder(enc_mu, decoder_domains, decoder_contents)\n",
    "\n",
    "        return transfers\n",
    "\n",
    "    def generate(self, codes, domains, contents):\n",
    "        \"\"\"\n",
    "        Generate images from Gaussian distributed codes.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            x = torch.cat((codes, domains, contents), dim=1)\n",
    "            x = self.decoder.linear(x)\n",
    "            x = self.decoder.reshape(x)\n",
    "            reconstructions = self.decoder.dec_conv_sequential(x)\n",
    "            self.train()\n",
    "            return reconstructions\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, data, num_domains, num_contents, latent_size, depth, out_channels, kernel_size, activation, downsampling, dropout, batch_norm):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.HW = {\"PACS\": 224, \"RMNIST\": 28}[self.data]\n",
    "        self.C = {\"PACS\": 3, \"RMNIST\": 1}[self.data]\n",
    "        self.num_domains = num_domains\n",
    "        self.num_contents = num_contents\n",
    "        self.latent_size = latent_size\n",
    "        self.depth = depth\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.downsampling = downsampling\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        if self.data == \"PACS\":\n",
    "            self.enc_conv_sequential = torch.nn.Sequential(\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.C + self.num_domains + self.num_contents,\n",
    "                    out_channels=self.out_channels[0],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=\"none\",\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [0], 224, 224)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[0],\n",
    "                    out_channels=self.out_channels[1],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=self.downsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [1], 112, 112)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[1],\n",
    "                    out_channels=self.out_channels[2],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=self.downsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [2], 56, 56)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[2],\n",
    "                    out_channels=self.out_channels[3],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=self.downsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [3], 28, 28)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[3],\n",
    "                    out_channels=self.out_channels[4],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=self.downsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [4], 14, 14)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[4],\n",
    "                    out_channels=self.out_channels[5],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=self.downsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [5], 7, 7)\n",
    "            )\n",
    "            self.flatten = torch.nn.Flatten()\n",
    "            self.get_mu = torch.nn.Linear(49 * self.out_channels[5], self.latent_size)\n",
    "            self.get_logvar = torch.nn.Linear(49 * self.out_channels[5], self.latent_size)\n",
    "        if self.data == \"RMNIST\":\n",
    "            self.enc_conv_sequential = torch.nn.Sequential(\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.C + self.num_domains + self.num_contents,\n",
    "                    out_channels=self.out_channels[0],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=\"none\",\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [0], 28, 28)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[0],\n",
    "                    out_channels=self.out_channels[1],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=self.downsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [1], 14, 14)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[1],\n",
    "                    out_channels=self.out_channels[2],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    downsampling=self.downsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [2], 7, 7)\n",
    "            )\n",
    "            self.flatten = torch.nn.Flatten()\n",
    "            self.get_mu = torch.nn.Linear(49 * self.out_channels[2], self.latent_size)\n",
    "            self.get_logvar = torch.nn.Linear(49 * self.out_channels[2], self.latent_size)\n",
    "\n",
    "    def block(self, depth, in_channels, out_channels, kernel_size, activation, downsampling=\"stride\", dropout=False, batch_norm=False):\n",
    "        seq_list = []\n",
    "        if isinstance(activation, torch.nn.SELU):\n",
    "            dropout = False\n",
    "            batch_norm = False\n",
    "        for i in range(depth):\n",
    "            seq = []\n",
    "            if i == 0: # downsampling in first layer of block\n",
    "                if downsampling == \"stride\":\n",
    "                    seq.append(torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                        padding=int((kernel_size-1)/2), stride=2, bias=not batch_norm))\n",
    "                    if batch_norm:\n",
    "                        seq.append(torch.nn.BatchNorm2d(num_features=out_channels))\n",
    "                    seq.append(activation)\n",
    "                    if dropout:\n",
    "                        seq.append(torch.nn.Dropout2d())\n",
    "                    seq_list += seq\n",
    "                elif downsampling == \"maxpool\":\n",
    "                    seq.append(torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                        padding=int((kernel_size-1)/2), stride=1, bias=not batch_norm))\n",
    "                    if batch_norm:\n",
    "                        seq.append(torch.nn.BatchNorm2d(num_features=out_channels))\n",
    "                    seq.append(activation)\n",
    "                    seq.append(torch.nn.MaxPool2d(kernel_size=2))\n",
    "                    if dropout:\n",
    "                        seq.append(torch.nn.Dropout2d())\n",
    "                    seq_list += seq\n",
    "                elif downsampling == \"none\":\n",
    "                    seq.append(torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                        padding=int((kernel_size-1)/2), stride=1, bias=not batch_norm))\n",
    "                    if batch_norm:\n",
    "                        seq.append(torch.nn.BatchNorm2d(num_features=out_channels))\n",
    "                    seq.append(activation)\n",
    "                    if dropout:\n",
    "                        seq.append(torch.nn.Dropout2d())\n",
    "                    seq_list += seq\n",
    "            else:\n",
    "                seq.append(torch.nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                    padding=int((kernel_size-1)/2), stride=1, bias=not batch_norm))\n",
    "                if batch_norm:\n",
    "                    seq.append(torch.nn.BatchNorm2d(num_features=out_channels))\n",
    "                seq.append(activation)\n",
    "                if dropout:\n",
    "                    seq.append(torch.nn.Dropout2d())\n",
    "                seq_list += seq\n",
    "        return seq_list\n",
    "\n",
    "    def forward(self, images, domains, contents):\n",
    "        \"\"\"\n",
    "        Calculates latent-space encodings for the given images in the form p(z | x).\n",
    "\n",
    "        images: Tensor of shape (batch_size, channels, height, width)\n",
    "        domains: Tensor of shape (batch_size, num_domains)\n",
    "        contents: Tensor of shape (batch_size, num_contents)\n",
    "        \"\"\"\n",
    "        domain_panels = torch.ones(size=(images.shape[0], self.num_domains, self.HW, self.HW)).to(\n",
    "            images.device) * domains.view(images.shape[0], self.num_domains, 1, 1)\n",
    "        content_panels = torch.ones(size=(images.shape[0], self.num_contents, self.HW, self.HW)).to(\n",
    "            images.device) * contents.view(images.shape[0], self.num_contents, 1, 1)\n",
    "\n",
    "        x = torch.cat((images, domain_panels, content_panels), dim=1)\n",
    "        x = self.enc_conv_sequential(x)\n",
    "        x = self.flatten(x)\n",
    "        enc_mu = self.get_mu(x)\n",
    "        enc_logvar = self.get_logvar(x)\n",
    "\n",
    "        return enc_mu, enc_logvar\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, data, num_domains, num_contents, latent_size, depth, out_channels, kernel_size, activation, upsampling, dropout, batch_norm, no_bn_last):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.HW = {\"PACS\": 224, \"RMNIST\": 28}[self.data]\n",
    "        self.C = {\"PACS\": 3, \"RMNIST\": 1}[self.data]\n",
    "        self.num_domains = num_domains\n",
    "        self.num_contents = num_contents\n",
    "        self.latent_size = latent_size\n",
    "        self.depth = depth\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.upsampling = upsampling\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        self.no_bn_last = no_bn_last\n",
    "        self.linear = torch.nn.Linear(\n",
    "            self.latent_size + self.num_domains + self.num_contents, 49 * self.out_channels[0])\n",
    "        if self.data == \"PACS\":\n",
    "            self.dec_conv_sequential = torch.nn.Sequential(\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[0],\n",
    "                    out_channels=self.out_channels[1],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=\"none\",\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm,\n",
    "                ),  # (N, [1], 7, 7)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[1],\n",
    "                    out_channels=self.out_channels[2],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=self.upsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm,\n",
    "                ),  # (N, [2], 14, 14)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[2],\n",
    "                    out_channels=self.out_channels[3],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=self.upsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [3], 28, 28)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[3],\n",
    "                    out_channels=self.out_channels[4],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=self.upsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [4], 56, 56)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[4],\n",
    "                    out_channels=self.out_channels[5],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=self.upsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [5], 112, 112)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[5],\n",
    "                    out_channels=self.C,\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=self.upsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm,\n",
    "                    last_block=self.no_bn_last\n",
    "                ),  # (N, self.C, 224, 224)\n",
    "            )\n",
    "        if self.data == \"RMNIST\":\n",
    "            self.dec_conv_sequential = torch.nn.Sequential(\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[0],\n",
    "                    out_channels=self.out_channels[1],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=\"none\",\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [1], 7, 7)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[1],\n",
    "                    out_channels=self.out_channels[2],\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=self.upsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm\n",
    "                ),  # (N, [2], 14, 14)\n",
    "                *self.block(\n",
    "                    depth=self.depth,\n",
    "                    in_channels=self.out_channels[2],\n",
    "                    out_channels=self.C,\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    activation=self.activation,\n",
    "                    upsampling=self.upsampling,\n",
    "                    dropout=self.dropout,\n",
    "                    batch_norm=self.batch_norm,\n",
    "                    last_block=self.no_bn_last\n",
    "                ),  # (N, self.C, 28, 28)\n",
    "            )\n",
    "\n",
    "    def block(self, depth, in_channels, out_channels, kernel_size, activation, upsampling=\"stride\", dropout=False, batch_norm=False, last_block=False):\n",
    "        seq_list = []\n",
    "        if isinstance(activation, torch.nn.SELU):\n",
    "            dropout = False\n",
    "            batch_norm = False\n",
    "        for i in range(depth):\n",
    "            seq = []\n",
    "            if i == 0: # upsampling in first layer of block\n",
    "                if upsampling == \"stride\":\n",
    "                    seq.append(torch.nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=4,\n",
    "                                        padding=int((kernel_size-1)/2), output_padding=0, stride=2, bias=not batch_norm))\n",
    "                    if batch_norm:\n",
    "                        if not (i == depth - 1 and last_block):\n",
    "                            seq.append(torch.nn.BatchNorm2d(num_features=out_channels))\n",
    "                    seq.append(activation)\n",
    "                    if dropout:\n",
    "                        if not (i == depth - 1 and last_block):\n",
    "                            seq.append(torch.nn.Dropout2d())\n",
    "                    seq_list += seq\n",
    "                elif upsampling == \"upsample\":\n",
    "                    seq.append(torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                        padding=int((kernel_size-1)/2), stride=1, bias=not batch_norm))\n",
    "                    if batch_norm:\n",
    "                        if not (i == depth - 1 and last_block):\n",
    "                            seq.append(torch.nn.BatchNorm2d(num_features=out_channels))\n",
    "                    seq.append(activation)\n",
    "                    seq.append(torch.nn.Upsample(scale_factor=2, mode=\"nearest\"))\n",
    "                    if dropout:\n",
    "                        if not (i == depth - 1 and last_block):\n",
    "                            seq.append(torch.nn.Dropout2d())\n",
    "                    seq_list += seq\n",
    "                elif upsampling == \"none\":\n",
    "                    seq.append(torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                        padding=int((kernel_size-1)/2), stride=1, bias=not batch_norm))\n",
    "                    if batch_norm:\n",
    "                        if not (i == depth - 1 and last_block):\n",
    "                            seq.append(torch.nn.BatchNorm2d(num_features=out_channels))\n",
    "                    seq.append(activation)\n",
    "                    if dropout:\n",
    "                        if not (i == depth - 1 and last_block):\n",
    "                            seq.append(torch.nn.Dropout2d())\n",
    "                    seq_list += seq\n",
    "            else:\n",
    "                seq.append(torch.nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                    padding=int((kernel_size-1)/2), stride=1, bias=not batch_norm))\n",
    "                if batch_norm:\n",
    "                    if not (i == depth - 1 and last_block):\n",
    "                        seq.append(torch.nn.BatchNorm2d(num_features=out_channels))\n",
    "                seq.append(activation)\n",
    "                if dropout:\n",
    "                    if not (i == depth - 1 and last_block):\n",
    "                        seq.append(torch.nn.Dropout2d())\n",
    "                seq_list += seq\n",
    "        return seq_list\n",
    "\n",
    "    def reshape(self, x):\n",
    "        return x.view(-1, self.out_channels[0], 7, 7)\n",
    "\n",
    "    def forward(self, codes, domains, contents):\n",
    "        \"\"\"\n",
    "        Calculates reconstructions of the given latent-space encodings.\n",
    "\n",
    "        codes: Tensor of shape (batch_size, latent_size)\n",
    "        domains: Tensor of shape (batch_size, num_domains)\n",
    "        contents: Tensor of shape (batch_size, num_contents)\n",
    "        \"\"\"\n",
    "        x = torch.cat((codes, domains, contents), dim=1)\n",
    "        x = self.linear(x)\n",
    "        x = self.reshape(x)\n",
    "        reconstructions = self.dec_conv_sequential(x)\n",
    "        return reconstructions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class SetToTanhRange(object):\n",
    "    \"\"\"\n",
    "    Shift torch.Tensor from [0, 1] to [-1, 1] range.\n",
    "    \"\"\"\n",
    "    def __call__(self, sample):\n",
    "        return 2.0 * sample - 1.0\n",
    "\n",
    "    \n",
    "def collate_function(batch):\n",
    "    images = []\n",
    "    domains = []\n",
    "    contents = []\n",
    "    for i in range(batch[0][1][0].shape[0]):\n",
    "        for minibatch in batch:\n",
    "            images.append(minibatch[0][i])\n",
    "            domains.append(minibatch[1][i])\n",
    "            contents.append(minibatch[2][i])\n",
    "    images = torch.stack(images)\n",
    "    domains = torch.stack(domains)\n",
    "    contents = torch.stack(contents)\n",
    "    return images, domains, contents\n",
    "\n",
    "\n",
    "class RMNISTDataset(Dataset):\n",
    "    def __init__(self, root, mode, domains, contents):\n",
    "        \"\"\"\n",
    "        root: str, root folder where RMNIST is located\n",
    "        mode: str, choose one: \"train\", \"val\" or \"test\"\n",
    "        domains: list of int [0, 15, 30, 45, 60, 75]\n",
    "        contents: list of int [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.domains = domains\n",
    "        self.contents = contents\n",
    "        self.domain_dict = {domain: torch.LongTensor([i]) for i, domain in enumerate(self.domains)}\n",
    "        self.content_dict = {content: torch.LongTensor([i]) for i, content in enumerate(self.contents)}\n",
    "        self.data_dir = f\"../{root}/RMNIST_{mode}\"\n",
    "        self.image_data = {domain: [] for domain in self.domains}\n",
    "        self.domain_data = {domain: [] for domain in self.domains}\n",
    "        self.content_data = {domain: [] for domain in self.domains}\n",
    "        for domain in os.listdir(f\"{self.data_dir}\"):\n",
    "            domain = int(domain)\n",
    "            image_data_ = ()\n",
    "            domain_data_ = ()\n",
    "            content_data_ = ()\n",
    "            if domain in self.domains:\n",
    "                for content in os.listdir(f\"{self.data_dir}/{domain}\"):\n",
    "                    content = int(content)\n",
    "                    if content in self.contents:\n",
    "                        imgs = torch.load(f\"{self.data_dir}/{domain}/{content}/data.pt\")\n",
    "                        image_data_ += (imgs,)\n",
    "                        domain_data_ += (torch.nn.functional.one_hot(self.domain_dict[domain], num_classes=len(self.domains)).view(1, -1),) * imgs.shape[0]\n",
    "                        content_data_ += (torch.nn.functional.one_hot(self.content_dict[content], num_classes=len(self.contents)).view(1, -1),) * imgs.shape[0]\n",
    "                image_data_ = torch.cat(image_data_, dim=0)\n",
    "                domain_data_ = torch.cat(domain_data_, dim=0)\n",
    "                content_data_ = torch.cat(content_data_, dim=0)\n",
    "                torch.manual_seed(17 + domain)\n",
    "                shuffle_inds = torch.randperm(len(image_data_))\n",
    "                image_data_ = image_data_[shuffle_inds]\n",
    "                domain_data_ = domain_data_[shuffle_inds]\n",
    "                content_data_ = content_data_[shuffle_inds]\n",
    "                self.image_data[domain] = image_data_\n",
    "                self.domain_data[domain] = domain_data_\n",
    "                self.content_data[domain] = content_data_\n",
    "        self.transform = self.get_transform()\n",
    "\n",
    "    def get_transform(self):\n",
    "        transform = transforms.Compose([\n",
    "            SetToTanhRange(),\n",
    "        ])\n",
    "        return transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return max([len(self.image_data[domain]) for domain in self.domains])\n",
    "\n",
    "    def __getitem__(self, idx):    \n",
    "        images = []\n",
    "        domains = []\n",
    "        contents = []\n",
    "        for d in self.domains:\n",
    "            if idx >= len(self.image_data[d]):\n",
    "                idx = torch.randint(low=0, high=len(self.image_data[d]), size=(1,)).item()\n",
    "            image = self.transform(self.image_data[d][idx])\n",
    "            domain = self.domain_data[d][idx]\n",
    "            content = self.content_data[d][idx]\n",
    "            images.append(image)\n",
    "            domains.append(domain)\n",
    "            contents.append(content)\n",
    "        return images, domains, contents\n",
    "\n",
    "        \n",
    "class BalancedRMNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, root, domains, contents, batch_size, num_workers, shuffle_all=False):\n",
    "        \"\"\"\n",
    "        root: str, root folder where RMNIST is located\n",
    "        domains: list of int [0, 15, 30, 45, 60, 75]\n",
    "        contents: list of int [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        batch_size: int, batch_size to use for the dataloaders\n",
    "        num_workers: int, how many workers to use for the dataloader\n",
    "        shuffle_all: bool, if True val and test dataloaders are shuffled as well\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.domains = domains\n",
    "        self.contents = contents\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.shuffle_all = shuffle_all\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage in (None, \"fit\"):\n",
    "            self.rmnist_train = RMNISTDataset(root=self.root, mode=\"train\", domains=self.domains, contents=self.contents)\n",
    "            self.rmnist_val = RMNISTDataset(root=self.root, mode=\"val\", domains=self.domains, contents=self.contents)\n",
    "        if stage in (None, \"test\"):\n",
    "            self.rmnist_test = RMNISTDataset(root=self.root, mode=\"test\", domains=self.domains, contents=self.contents)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.rmnist_train, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, collate_fn=collate_function)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.rmnist_val, batch_size=self.batch_size, shuffle=self.shuffle_all, num_workers=self.num_workers, collate_fn=collate_function)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.rmnist_test, batch_size=self.batch_size, shuffle=self.shuffle_all, num_workers=self.num_workers, collate_fn=collate_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"../logs/experiment/rmnist_meta4/version_0/checkpoints/last.ckpt\"\n",
    "data = \"RMNIST\"\n",
    "num_domains = 5\n",
    "num_contents = 10\n",
    "latent_size = 512\n",
    "lr = 1e-4\n",
    "depth = 2\n",
    "out_channels = [256, 512, 1024]\n",
    "kernel_size = 3\n",
    "activation = torch.nn.ELU()\n",
    "downsampling = \"stride\"\n",
    "upsampling = \"upsample\"\n",
    "dropout = False\n",
    "batch_norm = True\n",
    "loss_mode = \"elbo\"\n",
    "lamb = 0.01\n",
    "no_bn_last = True\n",
    "\n",
    "\n",
    "\n",
    "model = CVAE_v3.load_from_checkpoint(ckpt_path, num_domains=num_domains, num_contents=num_contents,\n",
    "                                     latent_size=latent_size, lr=lr, depth=depth, out_channels=out_channels,\n",
    "                                     kernel_size=kernel_size, activation=activation, downsampling=downsampling,\n",
    "                                     upsampling=upsampling, dropout=dropout, batch_norm=batch_norm, loss_mode=loss_mode,\n",
    "                                     lamb=lamb, no_bn_last=no_bn_last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = [0, 15, 30, 45, 60] \n",
    "contents = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \n",
    "dm = BalancedRMNISTDataModule(root=\"data\", domains=domains, contents=contents, batch_size=2, num_workers=0)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dm.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          ...,\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "          [-1., -1., -1.,  ..., -1., -1., -1.]]]]), tensor([[1, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1]]), tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = batch[0]\n",
    "domains = batch[1]\n",
    "contents = batch[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = model.reconstruct(images, domains, contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.) tensor(1.)\n",
      "tensor(-0.7525, grad_fn=<MinBackward1>) tensor(1.4866, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(images.min(), images.max())\n",
    "print(reconstructions.min(), reconstructions.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7930ccaf1029f9b8696599c8d114a3df4eb78a4984b726cde572031570791305"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('baver1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
